---
title: "Final Data Analysis Project"
date: "See Parts for Write-Up due Dates"
output:
  pdf_document: default
  html_document:
    df_print: paged
    fig_caption: TRUE
header-includes:  
  \usepackage{float} 
  \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


For this project you will take the role of a consultant hired by an Art historian to explore what drove prices of paintings in 18th century Paris.  They have provided you with auction price data from 1764-1780 on the sales (seller/buyer), painter, and other characteristics of paintings. 

## About the Data Analysis Project

The art historian would like to know what factors drove prices of painting, which paintings might be overvalued and which are undervalued.   It is up to you to decide what methods you want to use (frequentist or Bayesian or a combination) to answer these questions, and implement them to help to identify undervalued and overvalued paintings, as well as which features and possible interactions are at play.


You will have three data sets: a subset for training, a subset for testing, and a third subset for validation. You will be asked to do data exploration and build your model (or models) initially using only the training data. Then, you will test your model on the testing data, and finally validate using the validation data. We are challenging you to keep your analysis experience realistic, and in a realistic scenario you would not have access to all three of these data sets at once.  You will be able to see on our scoreboard how well your team is doing based on its predictive performance on the testing data.  After your project is turned in you will see the final score on the validation set.   

All members of the team should contribute equally and may be asked to answer any questions about the analysis at the final presentation.

*For your analysis create a new Rmd named "project-I.Rmd" for part I
and update accordingly rather than editing this.  Your write up should not have any of the instructions, for example. Figures should be labeled appropriately and report numbers using significant digits.  This file may be updated so do not edit this document.*

## Code:

In your write up your code should be hidden (`echo = FALSE`) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your Rmd file I should be able to obtain the results you presented.   If there is any code that you wish to highlight you may included it, but it should contribute significantly to your write up that should be directed to the art historian.

see Due dates in Sakai/Calendar for submissions

### Read in Training Data

To get started read in the training data:

```{r read-data, echo=TRUE}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
```

The Code Book is in the file `paris_paintings.md` provides more information about the data.

## Part I: Simple Model 

```{r packages}
suppressMessages(library(GGally))
suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(mice))
# need to delete if choose not to show rf process
suppressMessages(library(randomForest))
suppressMessages(library(caret))
suppressMessages(library(knitr))
```

### EDA

Using EDA and any numerical summaries get to know the data -  identify what you might consider the 10 best variables for predicting `logprice` using scatterplots with other variables represented using colors or symbols, scatterplot matrices or conditioning plots.

```{r}
# collinearity
table(paintings_train$Interm, paintings_train$type_intermed)
table(paintings_train$mat, paintings_train$materialCat)
```

Therefore, we can drop `Interm` and `materialCat` due to perfect collinearity.

```{r, echo = TRUE}
# missing randomly or not
missing = lm(paintings_train$logprice ~ paintings_train$winningbiddertype=="")
summary(missing)
```

This indicates that `winningbiddertype` is not missing purely randomly and that there is a strong case for selective non-response. Similarly, `endbuyer`, `type_intermed`, `material` and `mat` are not missing purely randomly either. So we impute "n/a" into `winningbiddertype`, `endbuyer`, `type_intermed`, `Shape`, `material` and `mat` to replace "" and "-" as a new category.

```{r clean data, warning = FALSE}
# mutate a variable that represents the number of paintings of a certain author and a certain winning bidder during this period
nauthors <- count(paintings_train, authorstandard)
paintings_train_new <- merge(paintings_train, nauthors, by = "authorstandard")
nbidders <- count(paintings_train, winningbidder)
paintings_train_new <- merge(paintings_train_new, nbidders, by = "winningbidder")
colnames(paintings_train_new)[60:61] <- c("n_author", "n_bidder")
# mutate a variable that represents ratio of height and width, which equals 1 for circle and square
paintings_train_new$ratio <- paintings_train_new$Height_in/paintings_train_new$Width_in
paintings_train_new[which(paintings_train_new$Diam_in != 0), 62] <- 1
# replace blank space and NA of categorical variables with n/a
paintings_train_new[, c(6, 8:11, 14:28, 30:32, 34:59)] <- as.data.frame(sapply(paintings_train_new[, c(6, 8:11, 14:28, 30:32, 34:59)], function(x) ifelse(x == "" | x == "-", "n/a", x)))
# clean position
paintings_train_new <- paintings_train_new[-which(paintings_train_new[, 5] > 1),]
# clean Shape
paintings_train_new[which(paintings_train_new$Shape == "ronde"), 28] <- "round"
paintings_train_new[which(paintings_train_new$Shape == "ovale"), 28] <- "oval"
```

We can drop `price` and `count` based on their meanings, with the former being another type of our response and the latter being completely useless since it is the same for all observations.

Besides, `sale`, `lot`, `subject`, `author` and `material` have too many levels, and thus they are not selected.

In addition, we have alrealy built the variables that contains the useful information of `authorstandard` and `winningbidder`, so we can drop them as well. Similarly, we can drop `Height_in`, `Width_in`, `Surface_Rect`, `Diam_in` and `Surface_Rnd`.

```{r}
# choose variables
paintings_train_new <- paintings_train_new %>% 
  select(-winningbidder, -authorstandard, -sale, -lot, -price, -count, -subject, -author, -Interm, -Height_in, -Width_in, -Surface_Rect, -Diam_in, -Surface_Rnd, -material, -materialCat)
```

Since there are some missing values in variable `Surface` and `ratio`, we need to deal with them.

```{r, echo = TRUE}
missing_Surface <- lm(paintings_train_new$logprice ~ is.na(paintings_train_new$Surface))
summary(missing_Surface)
missing_ratio <- lm(paintings_train_new$logprice ~ is.na(paintings_train_new$ratio))
summary(missing_ratio)
```

This indicates that `Surface` and `ratio` are not missing completely randomly, so we cannot just delete the missing rows. Instead, we choose to do the imputation using mice command.

```{r, cache = TRUE}
# imputate missing
set.seed(103)
imputed <- mice(paintings_train_new)
paintings_train_new <- mice::complete(imputed)
```

```{r, fig.height = 24, fig.width = 16, fig.align = "center"}
# scatterplot
par(mfrow = c(6, 4))
for(i in c(1:7, 9:25)){
  plot(paintings_train_new[,i], paintings_train_new[, 8], xlab = colnames(paintings_train_new)[i], ylab = "logprice", cex.axis = 2.5, cex.lab = 2.5)
}
```

```{r, fig.height = 24, fig.width = 16, fig.align = "center", fig.cap = "Plots of predictors versus logprice"}
# scatterplot
par(mfrow = c(6, 4))
for(i in 26:46){
  plot(paintings_train_new[,i], paintings_train_new[, 8], xlab = colnames(paintings_train_new)[i], ylab = "logprice", cex.axis = 2.5, cex.lab = 2.5)
}
```

Since the points of `Surface`, `nfigures`, `n_author` and `n_bidder` are clustered at the beginning of x axis, we can do log transformation to the corresponding predictors to see their relationship with `logprice`.

```{r}
summary(paintings_train_new[, c(15, 17, 44:45)])
```

Since the minimum values of `Surface` and `nfigures` are $0$, we need to add a positive value before doing the log transformation.

```{r, fig.height = 4, fig.width = 16, fig.align = "center", fig.cap = "Plots of some log predictors versus logprice"}
par(mfrow = c(1, 4))
for(i in c(15, 17, 44:45)){
  plot(log(paintings_train_new[, i] + 1), paintings_train_new[, 8], xlab = paste("log(", colnames(paintings_train_new)[i], ")", sep = ""), ylab = "logprice", cex.axis = 2, cex.lab = 2)
}
```

```{r, cache = TRUE, fig.cap = "Variable Importance Measures in Random Forests"}
# random forest
model_rf <- randomForest(logprice ~ .,data = paintings_train_new, mtry = 7, importance = TRUE)
varImpPlot(model_rf, n.var = 15, main = "Variable Importance Measures")
```


### Build your first model

In the first model predict the auction price `price` using the transformation `logprice` using at least 10 and up to 20 predictors and any interactions to build a model using linear regression. You may use stepwise model selection to simplify the model using AIC and/or BIC.  For reference, we will fit the null model to initialize the leaderboard, but replace model1 with your recommended model.

### version 1 (using random forest)

From the result of random forest, we can choose $14$ variables. (drop `diff_origin` since it's perfectly collinear with `origin_author` and `origin_cat`; drop `winningbiddertype` and `mat` since there are too many categories in them)

```{r, echo = TRUE, cache = TRUE}
# full model
model_full <- lm(logprice ~ (position + dealer + year + origin_author + origin_cat + endbuyer + type_intermed + Surface + paired + finished + lrgfont + n_author + n_bidder + ratio)^2, data = paintings_train_new)
# BIC
model_bic <- step(model_full, k = log(nobs(model_full)), direction = "both", trace = F)
summary(model_bic)
```

```{r, warning = FALSE}
# 10-fold cross validation
set.seed(1)
train(logprice ~ position + dealer + year + origin_author + origin_cat + endbuyer + type_intermed + Surface + paired + finished + lrgfont + n_author + n_bidder + ratio + position:Surface + position:n_author + dealer:year + year:endbuyer + year:n_author + year:n_bidder + origin_cat:n_author + Surface:paired + Surface:n_bidder + Surface:ratio, method = "lm", data = paintings_train_new, trControl = trainControl(method = "cv", number = 10))
```

10-fold cross validation RMSE $1.206108$.

### version 2 (using BIC)

```{r AIC, echo=TRUE, cache = TRUE}
# full model
model_full_2 <- lm(logprice ~ ., data = paintings_train_new)
# AIC
model_aic_2 <- step(model_full_2, k = 2, direction = "both", trace = F)
summary(model_aic_2)
```

Since there are more than $20$ predictors using AIC, we will turn to the more strict criterion BIC.

```{r BIC, echo = TRUE, cache = TRUE}
# BIC
model_bic_2 <- step(model_full_2, k = log(nobs(model_full)), trace = F)
summary(model_bic_2)
```

(drop `winningbiddertype` since there are too many categories in it)

```{r BIC with interaction, echo = TRUE, cache = TRUE}
# BIC with interaction
model_full_interaction <- lm(logprice ~ (dealer + year + origin_author + diff_origin + artistliving + Surface + engraved + prevcoll + paired + finished + lrgfont + lands_sc + still_life + discauth + n_author + ratio)^2, data = paintings_train_new)
model_bic_2 <- step(model_full_interaction, k = log(nobs(model_full_interaction)), trace = F)
summary(model_bic_2)
```

```{r, warning = FALSE}
# 10-fold cross validation
set.seed(1)
train(logprice ~ dealer + year + origin_author + diff_origin + artistliving + Surface + engraved + prevcoll + paired + finished + lrgfont + lands_sc + still_life + discauth + n_author + ratio + dealer:year + diff_origin:discauth + diff_origin:ratio + Surface:still_life + Surface:ratio + prevcoll:finished + paired:lrgfont + paired:ratio + finished:discauth + discauth:ratio, method = "lm", data = paintings_train_new, trControl = trainControl(method = "cv", number = 10))
```

10-fold cross validation RMSE $1.225015$.

Since $1.206108 < 1.225015$, we choose model_bic as our final model.

```{r, warning = FALSE, fig.height = 8, fig.width = 8, fig.align = "center", fig.cap = "Diagnostic Plots"}
model1 <- model_bic
# diagnostic plots
par(mfrow = c(2, 2))
plot(model1)
```

Save predictions and intervals. 

```{r predict-model1, echo = FALSE, warning = FALSE}
# prediction
# mutate a variable that represents the number of paintings of a certain author and a certain winning bidder during this period
nauthors <- count(paintings_test, authorstandard)
paintings_test_new <- merge(paintings_test, nauthors, by = "authorstandard")
nbidders <- count(paintings_test, winningbidder)
paintings_test_new <- merge(paintings_test_new, nbidders, by = "winningbidder")
colnames(paintings_test_new)[60:61] <- c("n_author", "n_bidder")
# mutate a variable that represents ratio of height and width, which equals 1 for circle and square
paintings_test_new$ratio <- paintings_test_new$Height_in/paintings_test_new$Width_in
paintings_test_new[which(paintings_test_new$Diam_in != 0), 62] <- 1
# replace blank space and NA of categorical variables with n/a
paintings_test_new[, c(6, 8:11, 14:28, 30:32, 34:59)] <- as.data.frame(sapply(paintings_test_new[, c(6, 8:11, 14:28, 30:32, 34:59)], function(x) ifelse(x == "" | x == "-", "n/a", x)))
# clean position
paintings_test_new[which(paintings_test_new[, 5] > 1), 5] <- paintings_test_new[which(paintings_test_new[, 5] > 1), 5]/100
# clean Shape
paintings_test_new[which(paintings_test_new$Shape == "ronde"), 28] <- "round"
paintings_test_new[which(paintings_test_new$Shape == "ovale"), 28] <- "oval"
# choose variables
paintings_test_new <- paintings_test_new %>% 
  select(-winningbidder, -authorstandard, -sale, -lot, -price, -count, -subject, -author, -Interm, -Height_in, -Width_in, -Surface_Rect, -Diam_in, -Surface_Rnd, -material, -materialCat)
# imputate missing
set.seed(103)
imputed <- mice(paintings_test_new)
paintings_test_new <- mice::complete(imputed)
predictions = as.data.frame(
  exp(predict(model1, newdata = paintings_test_new, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```


### Part I Write up *Last day to submit is Dec 7 by 5; accepted until Dec 6 (5 points off if late)*

Once you are satisfied with your model, provide a write up of your data analysis project in a new Rmd file/pdf file: `Part-I-Writeup.Rmd` by copying over salient parts of your R notebook. The written assignment consists of five parts:

1. Introduction: Summary of problem and objectives (5 points)

2. Exploratory data analysis (10 points): must include three correctly labeled graphs and an explanation that highlight the most important features that went into your model building.

3. Development and assessment of an initial model (10 points)

* Initial model: must include a summary table and an explanation/discussion for variable selection and overall amount of variation explained. 

* Model selection: must include a discussion

* Residual: must include residual plot(s) and a discussion.  

* Variables: must include table of coefficients and CI

```{r}
summary(model_bic)
anova(model_bic)
```
For the initial model, we use the random forest to choose the main predictors first due to the following reasons.
1. Tree regression consider interactions among predictors more comprehensively compared to using step to choose predictors. More specifically, if we directly use AIC/BIC to choose main predictors, these two methods may ignore some predictors that have significant interactions with other predictors. By contrast, random forest will choose the important predictors for us that also involve the interactions.
2. Trees are easy to understantd by people. Trees are much easier for us to explain the reasoning to non statisticians since our main job is to convince the Art historian.

Secondly, after we use random forest to find important variables, we still need to drop several predictors because they may have some potential problems
1. drop "diff_origin" since it's perfectly colinear with "origin_author" and "origin_cat"
2. drop "winningbiddertype" and "mat" since there are too many categories in them.

Thirdly, we put the left main predictors and all their interactions into a full model. Then we use BIC to
choose the important predictors and interactios for us.

Finally, in the initial model, we have 14 main predictors and 19 interactions. Roughly 63% variation of dependent variables are explained by this model. By looking at the anova table of the model, almost all of the variables are significant at the 5% level, which indicate the variables in the model are reasonable.


```{r, warning = FALSE, fig.height = 8, fig.width = 8, fig.align = "center", fig.cap = "Diagnostic Plots"}
par(mfrow = c(2, 2))
plot(model_bic)
```
In the Residual vs Fitted plot, there is only a slight curve at the end of the 0 line due to an outlier. Almost all points are randomly distributed around 0, which indicates no violation for the linearity assumption.

The Normal Q-Q plot indicates nearly perfect poins distribution. Almost all residuals follow a normal distribution.

In the Scale-Location plot, there is no pattern shows the violation of constant variation.

In the Residuals vs Leverage plot, there are neither actually influential nor potentially influential points.
Only several outliers exist.

Generally, the diagnostic plots tell us that the linear model we get fit the train data very well. 

```{r warning=FALSE}
coef=summary(model_bic)$coefficients
kable(data.frame(estimate=coef[,1],
                 CI_Low=coef[,1]-1.96*coef[,2],
                 CI_Up=coef[,1]+1.96*coef[,2]))
```

In the table above, we can see part of the variables have high estimates compared to others.
It may indicate the importance of the variables or the potential problems exist in the linear model.
There also exist several variables whose condifence inteval contain 0. These variables may either have positive or negative effects on the price. we will use more complicated model in the next part to improve the performance of the model.

4. Summary and Conclusions (10 points)

What is the (median) price for the "baseline" category if there are categorical or dummy variables in the model (add CI's)?  (be sure to include units!) Highlight important findings and potential limitations of your model.  Does it appear that interactions are important?  What are the most important variables and/or interactions?  Provide interprations of how the most important variables influence the (median) price giving a range (CI).  Correct interpretation of coefficients for the log model desirable for full points.

Provide recommendations for the art historian about features or combination of features to look for to find the most valuable paintings.

_Points will be deducted for code chunks that should not be included, etc._

*Upload write up  to Sakai any time before Dec 7th*


The baseline price is $e^{-334}$ livres, whcih is approximately 0 livres. But this value is not meaningful, as it represents the price of a painting without dealer, author, shape, etc. In our model, we find that dealer, endbuyer, year, and ratio are the four most affective predictors to the price based on the huge price change when choosing different predictor value. But some large coefficient value also indicate the limitation of the linear model. So we need to use the non-linear model to shrink the estimate value to get a biased model since not all the predictors are equally important.From the anova table, we can see lots of interactions are important to the reponse variable according to the p_value. But not all of them imply in this way. Some interactions are trvial to the price change. So part of the interactions are important. 

We think dealer and the interaction between dealer and year are the most predictor and interaction.
when the dear is L, the price will increase by $e^{171}$ to $e^{373}$.
...
...


###  Evaluation on test data for Part I

Once your write up is submitted, your models will be evaluated on the following criteria based on predictions  on the test data (20 points): 

* Bias:  Average (Yhat-Y)  positive values indicate the model tends to overestimate price (on average) while negative values indicate the model tends to underestimate price.

* Maximum Deviation:  Max |Y-Yhat| -  identifies the worst prediction  made in the validation data set.

* Mean Absolute Deviation:  Average |Y-Yhat| - the average error (regardless of sign).

* Root Mean Square Error: Sqrt Average (Y-Yhat)^2

* Coverage:  Average( lwr < Y < upr) 

In order to have a passing wercker badge, your file for predictions needs to be the same length as the test data, with three columns:  fitted values, lower CI and upper CI values in that order with names, *fit*, *lwr*, and *upr* respectively such as in the code chunk below. 

Save predictions and intervals.  
```{r predict-model-final, echo=FALSE, include=FALSE}
# change model1 or update as needed
predictions = as.data.frame(
  exp(predict(model1, newdata = paintings_test_new, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```

You will be able to see your scores on the score board.  They will be initialized by a prediction based on the mean in the training data.