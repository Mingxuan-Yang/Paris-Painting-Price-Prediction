---
title: "Part-I-Writeup"
author: "Team-FP03"
date: "2019/12/4"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r packages}
suppressMessages(library(knitr))
suppressMessages(library(GGally))
suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(mice))
```

```{r read-data}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
```

# Introduction

In this project, we are going to explore what factors drove the price of paintings in 18th century Paris, and thus to identify possible overvalued and undervalued paintings.  

The dataset we are going to analyze is a series of auction transactions of paintings in Paris, ranging from 1764 to 1780. This dataset mainly contains the following information:  

1. Sale data, this include basic information about painters, dealers, end buyers, transaction dates and prices;  
2. Characteristics of paintings, such as their sizes, materials, number of figures and themes.  

To address our problem, we devide this project into two parts:  

1. In the first part, we carried out an exploratory data analysis. The target of this section is to understand the composition of our dataset and identify potential important variables.  
2. In the second part, a simple linear regression model was fit to the data, aiming to confirm important variables and interactions from the model selection process and to prepare for fitting a more complex model.  

# Exploratory Data Analysis  

In this section, we are going to explore our dataset in the following way: we first investigate the variables in the dataset to find their characteristics and possible relationships among each other; then we check the scatterplots between the response and each variable to identify potential important predictors.  

## Variable investigation

First of all, we can remove a few variables from the list of potential predictors simply based on their definitions:  
Variable `price` is just the exponetial form of our target response `logprice`, and thus needs removing;  
Variable `count` is the same for all observations, therefore there's no point to use it in the model fitting.  

Besides these two, there exist quite a number of variables of interest:  

### Variables to impute

We've found that NA's exist in a lot of variables, and these NA's do not always indicate values missing completely at random. For example, from the R output below, we can see that `Surface` is not missing at random. Thus, instead of simply discarding observations containing NA's, we choose to impute the missing values with the observed ones.  
For variables with a lot of blank values such as `endbuyer`, `type_intermed`, `material` and `mat`, we impute "n/a" into them to create a new category.  

```{r, echo = FALSE}
missing_Surface <- lm(paintings_train$logprice ~ is.na(paintings_train$Surface))
summary(missing_Surface)
```


```{r}
# replace blank space and NA of categorical variables with n/a
paintings_train_new <- paintings_train
paintings_train_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)] <- as.data.frame(sapply(paintings_train_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)], function(x) ifelse(x == "" | x == "-", "n/a", x)))
```

### Variables to manipulate

Variable `position` indicates the position of lot in the catalogue and is expressed as percentages. However, the maximum value of it in the dataset can be as large as `r round(max(paintings_train$position), 2)`, which are obviously typos. Similarly, there are observations with a series of size variables such as `Surface` all equal to 0. As a result, observations with impossible `position` and `Surface` values are dropped.  
Besides, `Shape` variable has some weird values, such as `oval` vs. `ovale`, and `ronde` vs. `round`, which are probably typos and thus need fixing.  
Addtionally, if variables `origin_author` and `origin_cat` are known, the value of `diff_origin` is 100% certain. Also, `type_intermed` incorporates all information Thus, since the former two variables contain more specific information, we decide to drop `diff_origin`.  
In a similar manner, `Surface` should be known if `Diam_in`, or `Height_in` and `Width_in` are known at the same time. Also, note that `Surface` is the combination of `Surface_Rnd` and `Surface_Rect`. Thus, among all these variables mentioned, we keep just `Surface` in the model fitting process.  
Variables `authorstandard`, `author`, `subject`, `sale`, `lot`, and `material` have way too many distinct values. Also, the possible values for these variables are too complicated and we decide not to use them in this simple model. When fitting a more complex model, it may be a good idea to convert them into new variables.  
At last, in the dataset there exist strong correlations among some pairs of variables. For example, there is correlation between `Interm` & `type_intermed`, and `mat` & `materialCat`. In the following table, we display the contingency table for `Interm` vs. `type_intermed`, and as we can see, when `Interm` takes 0 `type_intermed` always takes `n/a`; when `Interm` takes 1, `type_intermed` takes other values. Thus, we decide to remove `Interm` and `materialCat`.  

```{r echo=FALSE}
# collinearity
table(paintings_train_new$Interm, paintings_train_new$type_intermed)
table(paintings_train_new$mat, paintings_train_new$materialCat)
```


## Important predictor identification

In this section we are going to evaluate scatter plots between our response `logprice` and each varaible after the manipulation from the previous part.  

```{r include=FALSE}
# clean position
paintings_train_new <- paintings_train_new[-which(paintings_train_new$position > 1),]

# clean Shape
paintings_train_new[which(paintings_train_new$Shape == "ronde"), 28] <- "round"
paintings_train_new[which(paintings_train_new$Shape == "ovale"), 28] <- "oval"

# choose variables
paintings_train_new <- paintings_train_new %>% 
  select(-winningbidder, -authorstandard, -sale, -lot, -price, -count, -subject, -author, -Interm, -Surface_Rect, -Surface_Rnd, -material, -materialCat, -Height_in, -Width_in, -Diam_in)

# delete repeating data
paintings_train_new <- unique(paintings_train_new)

# imputate missing
set.seed(103)
imputed <- mice(paintings_train_new, m = 5)
paintings_train_new <- mice::complete(imputed)

# clean Surface
paintings_train_new <- paintings_train_new[-which(paintings_train_new$Surface == 0),]
```


```{r, fig.height = 24, fig.width = 16, fig.align = "center", echo=FALSE}
# scatterplot
par(mfrow = c(6, 4))
for(i in c(1:7, 9:24)){
  plot(paintings_train_new[, i], paintings_train_new[, 8], xlab = colnames(paintings_train_new)[i], ylab = "logprice", cex.axis = 2, cex.lab = 2)
}
```

The **Figure 1** above displays the scatter plots between `logprice` and the first 24 variables in the dataset. Our target is to identify variables that show a strong relationship with the response. Bearing this in mind, it is easy to notice that variables `dealer`, `year`, `origin_author`, `prevcoll`, `endbuyer`, `type_intermed` and `Shape` appear to have the strongest relationship with `logprice`. In addition, variables such as `Surface` are clustered near the beginning of x axis, and thus we decide to apply log transformations on them and have a closer look afterwards.  

```{r, fig.height = 20, fig.width = 16, fig.align = "center", fig.cap = "Plots of predictors versus logprice", echo=FALSE}
# scatterplot
par(mfrow = c(5, 4))
for(i in 25:43){
  plot(paintings_train_new[, i], paintings_train_new[, 8], xlab = colnames(paintings_train_new)[i], ylab = "logprice", cex.axis = 2, cex.lab = 2)
}
```

**Figure 2** above display the scatter plots between `logprice` and the rest of the variables in the dataset. As we can see, most of the binary categorical variables fail to present a strong relationship with the response. The only exception is `lrgfont`, which corresponds to quite different response values at the two different levels.  


For `Surface`, we can do log transformation to the corresponding predictors to see their relationship with `logprice` at a greater detail in **Figure 3**.  

```{r, fig.height = 4, fig.width = 4, fig.align = "center", fig.cap = "Plots of log Surface versus logprice"}
plot(log(paintings_train_new[, 15]), paintings_train_new[, 8], xlab = paste("log(", colnames(paintings_train_new)[15], ")", sep = ""), ylab = "logprice")
```

```{r}
# log transformation to Surface
paintings_train_new$Surface <- log(as.numeric(paintings_train_new$Surface))
colnames(paintings_train_new)[15] <- "log_Surface"
```

As we can see from **Figure 3**, there seem to be a weak relationship between `logprice` and log-transformed `Surface`. Intuitively, the surface of paintings should indeed be correlated to their prices.  

In conclusion, after our manipulation with the dataset and inspection of the relationships between response and each variable, we reckon that variables `dealer`, `year`, `origin_author`, `prevcoll`, `endbuyer`, `type_intermed`, `Shape`, `lrgfont` and the log transformation of `Surface` are the most important variables in terms of scatter plots and their definitions. However, we need formal model fitting and selection process to decide the variables and interactions to use.  


# Model fitting

In this section, we are going to present the development and assessment of our simple model.

```{r cache=TRUE}
model_test <- lm(logprice ~ (dealer + year + origin_author + origin_cat + authorstyle + endbuyer + type_intermed + log_Surface + finished + lrgfont + Shape + prevcoll)^2, data = paintings_train_new)
model_bic2 <- step(model_test, k = log(nobs(model_test)), direction = "both", trace = F)
model1=model_bic2
```

First of all, we display the summary and anova table for our final model
```{r}
summary(model1)
anova(model1)
```


The following is the process of model building:  

First, for our initial model, we decide to incorporate all the important predictors identified in EDA, and then add some extra predictors for the following reasons:

1.`origin_cat`: when a painting was created by artists who were not well-known, then the origin of paintings based on dealers’ classification is the only way bidders get to know the origin of paintings, therefore we think origin_cat would be helpful for our model beside origin_author. And we use an anova test to check that.
```{r}
ano_data=paintings_train_new%>%select(logprice,origin_cat)
summary(aov(logprice~origin_cat,data=ano_data))
```

Here, we set the `logprice` to multiple groups according to `origin_cat`, and use anova to compare whether their group means are the same or not. Since the p_value is smaller than 0.05. we can state that the means are differnet across groups. So we decide to use `origin_author`.


2.`finished`: we believe that whether a painting is finished or not will affect the the price of the sale.
Also, the plot of `finished` in EDA can also prove our thinking. So we choose this predictor based on both commen sense and plot analysis.

Secondly, we put the chosen main predictors and all their interactions into a full model. Then we use BIC to choose the important predictors and interactios for us.

Finally, in the simple model, we have 8 main predictors and 1 interaction. Roughly 60% variation of dependent variables are explained by this model. By looking at the anova table of the model, all of the variables are significant at the 5% level, which indicate the variables in the model are reasonable.

```{r, warning = FALSE, fig.height = 8, fig.width = 8, fig.align = "center", fig.cap = "Diagnostic Plots"}
par(mfrow = c(2, 2))
plot(model1)
```
In **Figure 4**, the Residual vs Fitted plot, there is only a slight curve at the begining of the 0-level horizontal line, which is not a serious problem. Almost all points are randomly distributed around 0, indicating no significant violation for the linearity assumption.

The Normal Q-Q plot indicates nearly perfect distribution. Almost all residuals follow a normal distribution.

In the Scale-Location plot, the red line suggest that there is a small pattern for the resuduals. The absolute value of residuals will increase first and then decrease. But the points are very well randomly distributed around the 0 line, So the violation of constant variance is not significant enough to be very concerning. 

In the Residuals vs Leverage plot, there are neither actually influential nor potentially influential ones.

Generally, the diagnostic plots tell us that the linear model we get fits the training data very well and does not violate any assumptions. 

```{r warning=FALSE}
coef <- summary(model1)$coefficients
kable(data.frame(estimate=coef[,1],
                 CI_Low=coef[,1]-1.96*coef[,2],
                 CI_Up=coef[,1]+1.96*coef[,2]))
```

In the table above, we can see that part of the variables have high estimates compared to others.
It may indicate the importance of the variables or the potential problems exist in the linear model.
There also exist several variables whose condifence inteval contain 0. These variables may either have positive or negative effects on the price. we will use more complicated model in the next part to improve the performance of the model.

# Summary and Conclusions

In our final model, the baseline price is $e^{-150}$ livres, which is approximately 0 livres. It represents the price of a painting under baseline categories for all categorical variables, such as `dealer` and `endbuyer`,etc.

According to the coefficient table we get above, predictor `year` and `endbuyer` have huge impact on the price sale. For `year`, althouth its coefficient is not large compared to others, the big numeric value itself will have impact on the price. Beside `year`, for the dummy variables, `endbuyer` is another important predictor that affect the price most.

The two most important variables are `year` and `endbuyer`. And the only interaction we have is the interaction between `year` and `endbuyer`. So it's natural to say the interaction is also improtant.

Our model also has limitations. we choose the main predictors mostly from EDA and by ourselves so we may ignore some important predictors. In our simple model, predictors `year`, `endbuyer`, and `year:endbuyer` look a little overly important compard to all other variables, Which means to a  certain degree we can just predict the price by using 3 variables. it's questionalble for such a large data set. Besides, we only use the linear model to fit the data, resulting in a few large coefficents and standard deviations. Furthermore, the big estimated coefficients make us hard to interpreate the model to the art historian. Thus, we may use nonlinear model to shrink the coefficients in the next part. There may even exist some more complicated relationships in the data such as polynomial. We still need to explore that.

For every one year after the previous year, we expect price of the painting will be $e^{0.09}$ times higher, and we are 95% confident that the fluction is between $e^{-0.07}$ to $e^{0.24}$, which is from 0.93 to 1.27.

Given all other conditions unchanged(eg:same dealer, same year,same origin,etc.), we expect the price of painting will be $e^{-91}$ times higher if the buyer is a collector. And we are 95% confident that the price fluction will be between $e^{-375}$ and $e^{193}$ times higher.

Given all other conditions unchanged(eg:same dealer, same year,same origin,etc.), we expect the price of painting will be $e^{13}$ times higher if the buyer is a dealer And we are 95% confident that the price fluction will be between $e^{-272}$ and $e^{398}$ times higher. 

Given all other conditions unchanged(eg:same dealer, same year,same origin,etc.), we expect the price of painting will be $e^{-247}$ times higher if the buyer is expert organizing the sale. And we are 95% confident that the price fluction will be between $e^{-537}$ and $e^{43}$ times higher. 

Given all other conditions unchanged(eg:same dealer, same year,same origin,etc.), we expect the price of painting will be $e^{-34}$ times higher if the buyer is unknown person. And we are 95% confident that the price fluction will be between $e^{-318}$ and $e^{250}$ times higher. 

Given all other conditions unchanged(eg:same dealer, same year,same origin,etc.), we expect the price of painting will be $e^{-40}$ times higher if the buyer is person without information. And we are 95% confident that the price fluction will be between $e^{-329}$ and $e^{248}$ times higher. 

So we sugget the art historians that the painting bought by dealer with larger year will have a high value.  

```{r predict-model1, echo = FALSE, warning = FALSE}
# test data
paintings_test_new <- paintings_test

# replace blank space and NA of categorical variables with n/a
paintings_test_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)] <- as.data.frame(
  sapply(paintings_test_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)],
         function(x) ifelse(paste(x) == "" | paste(x) == "-",
                            "n/a", paste(x))))

# clean position
paintings_test_new[which(paintings_test_new$position > 1), 3] <-
  paintings_test_new[which(paintings_test_new$position > 1), 3]/100

# clean Shape
paintings_test_new[which(paintings_test_new$Shape == "ronde"), 28] <-
  "round"
paintings_test_new[which(paintings_test_new$Shape == "ovale"), 28] <-
  "oval"

# choose variables
paintings_test_new <- paintings_test_new %>% 
  select(-winningbidder, -authorstandard, -sale, -lot, -price, -count,
         -subject, -author, -Interm, -Height_in, -Width_in, -Diam_in,
         -Surface_Rect, -Surface_Rnd, -material, -materialCat)

# clean Surface
paintings_test_new[which(paintings_test_new$Surface == 0), 15] <- NA

# imputate missing
set.seed(103)
imputed <- mice(paintings_test_new, m = 5)
paintings_test_new <- mice::complete(imputed)

# log transformation to Surface
paintings_test_new$Surface <- log(paintings_test_new$Surface)
colnames(paintings_test_new)[15] <- "log_Surface"
```


```{r predict-model-final, echo=FALSE, include=FALSE}
# change model1 or update as needed
predictions <- as.data.frame(
  exp(predict(model1, newdata = paintings_test_new, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```
